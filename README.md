# BIG DATA ANALYSIS

*COMPANY*: CODTECH IT SOLUTIONS

*NAME*: MAITHRI MUDALIAR

*INTERN ID*: CT04DH1025

*DOMAIN*: DATA ANALYSIS

*DURATION*: 4 WEEKS

*MENTOR*: NEELA SANTOSH

# DATA ANALYSIS TASK - FAKE SALES DATA USING DASK FOR SCALABILITY

THIS PROJECT DEMONSTRATES HOW TO HANDLE AND ANALYZE LARGE-SCALE SALES DATA USING DASK, A POWERFUL PYTHON LIBRARY FOR PARALLEL COMPUTING. 
THE DATASET USED IS A SYNTHETIC (FAKE) SALES DATASET CREATED FOR TESTING THE SCALABILITY OF DATA PROCESSING WORKFLOWS.

THE OBJECTIVE OF THIS TASK IS TO:
- PROCESS LARGE DATASETS THAT CANNOT BE HANDLED EFFICIENTLY WITH PANDAS
- DEMONSTRATE THE ADVANTAGES OF DASK FOR OUT-OF-MEMORY DATA
- PERFORM BASIC EXPLORATORY DATA ANALYSIS (EDA) USING DASK DATAFRAMES
- IDENTIFY KEY SALES PATTERNS SUCH AS TOP-SELLING PRODUCTS, CITIES WITH MOST ORDERS, AND MONTHLY SALES TRENDS

KEY FEATURES OF THE PROJECT:
- DATA LOADED AND PROCESSED USING DASK FOR BETTER PERFORMANCE
- COMPUTATION PERFORMED IN CHUNKS TO AVOID MEMORY OVERLOAD
- SALES AGGREGATIONS DONE ON COLUMNS LIKE ORDER DATE, PRODUCT, QUANTITY, AND REVENUE
- VISUALIZATION USED TO SUPPORT FINDINGS

TOOLS & TECHNOLOGIES USED:
- PYTHON
- DASK
- JUPYTER NOTEBOOK
- FAKE SALES CSV DATASET

THIS PROJECT SHOWS MY ABILITY TO SCALE DATA ANALYSIS USING DASK AND HANDLE DATASETS THAT ARE TOO LARGE FOR CONVENTIONAL METHODS LIKE PANDAS. 
IT ALSO HIGHLIGHTS HOW TO OPTIMIZE DATA ANALYTICS WORKFLOWS IN REAL-WORLD SCENARIOS WHERE DATA SIZE IS A LIMITING FACTOR.

# DATA

[fake_sales_data-checkpoint.csv](https://github.com/user-attachments/files/21051494/fake_sales_data-checkpoint.csv)
